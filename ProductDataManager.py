import streamlit as st
import pandas as pd
from docx import Document
import io
import re
import json
from datetime import datetime
import requests
import pdfplumber

class OllamaClient:
    def __init__(self):
        self.base_url = "http://localhost:11434"
        self.model = "mistral:7b-instruct"
    
    def is_available(self):
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def generate_response(self, prompt):
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False,
            "options": {"temperature": 0.1, "num_predict": 1000}
        }
        
        try:
            response = requests.post(f"{self.base_url}/api/generate", json=payload, timeout=120)
            if response.status_code == 200:
                return response.json().get("response", "").strip()
            return None
        except:
            return None

class DataLoader:
    def __init__(self):
        self.products_df = None
    
    def load_products_from_files(self, uploaded_files):
        try:
            all_dfs = []
            for uploaded_file in uploaded_files:
                file_name = uploaded_file.name.lower()
                
                if file_name.endswith('.csv'):
                    df = self._load_csv(uploaded_file)
                elif file_name.endswith('.pdf'):
                    df = self._load_pdf(uploaded_file)
                elif file_name.endswith('.json'):
                    # JSON —Ñ–∞–π–ª—ã —Å —Ç–æ–≤–∞—Ä–∞–º–∏ –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –¥–ª—è –∫–∞—Ç–∞–ª–æ–≥–∞
                    st.warning(f"JSON —Ñ–∞–π–ª—ã —Å –∑–∞–ø—Ä–æ—Å–∞–º–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ. –§–∞–π–ª {uploaded_file.name} –ø—Ä–æ–ø—É—â–µ–Ω.")
                    continue
                else:
                    st.error(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {uploaded_file.name}")
                    continue
                
                if df is not None:
                    all_dfs.append(df)
            
            if all_dfs:
                self.products_df = pd.concat(all_dfs, ignore_index=True)
                self.products_df = self.products_df.drop_duplicates(subset=['–¢–æ–≤–∞—Ä'])
                return True
            return False
                
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤: {str(e)}")
            return False
    
    def _load_csv(self, uploaded_file):
        df = pd.read_csv(uploaded_file)
        if '–¢–æ–≤–∞—Ä' not in df.columns:
            for col in df.columns:
                if any(keyword in str(col).lower() for keyword in ['—Ç–æ–≤–∞—Ä', 'product', '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ']):
                    df = df.rename(columns={col: '–¢–æ–≤–∞—Ä'})
                    break
            else:
                df.columns = ['–¢–æ–≤–∞—Ä'] + list(df.columns[1:])
        return df
    
    def _load_pdf(self, uploaded_file):
        try:
            with pdfplumber.open(uploaded_file) as pdf:
                text_content = []
                for page in pdf.pages:
                    text = page.extract_text()
                    if text:
                        text_content.append(text)
            
            full_text = "\n".join(text_content)
            
            # –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ PDF
            product_lines = []
            
            # –†–∞–∑–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å—Ç—Ä–æ–∫–∏ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–æ–∫—É
            lines = full_text.split('\n')
            current_line = ""
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç "—Ä—É–±", —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ç–æ–≤–∞—Ä
                if '—Ä—É–±' in line.lower():
                    # –ï—Å–ª–∏ —É –Ω–∞—Å –µ—Å—Ç—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–∞—è –ø—Ä–µ–¥—ã–¥—É—â–∞—è —Å—Ç—Ä–æ–∫–∞, –¥–æ–±–∞–≤–ª—è–µ–º –µ–µ –∫ —Ç–µ–∫—É—â–µ–π
                    if current_line:
                        line = current_line + " " + line
                        current_line = ""
                    
                    # –û—á–∏—â–∞–µ–º —Å—Ç—Ä–æ–∫—É –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
                    clean_line = re.sub(r'\s+', ' ', line.strip())
                    product_lines.append(clean_line)
                else:
                    # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç "—Ä—É–±", –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —á–∞—Å—Ç—å—é –æ–ø–∏—Å–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–∞
                    # –ù–∞–∫–æ–ø–ª—è–µ–º –µ–µ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
                    current_line = line
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—É—é —Å—Ç—Ä–æ–∫—É, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
            if current_line and '—Ä—É–±' in current_line.lower():
                clean_line = re.sub(r'\s+', ' ', current_line.strip())
                product_lines.append(clean_line)
            
            if product_lines:
                df = pd.DataFrame(product_lines, columns=['–¢–æ–≤–∞—Ä'])
                return df
            else:
                st.warning(f"–í PDF —Ñ–∞–π–ª–µ {uploaded_file.name} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤ —Å —Ü–µ–Ω–∞–º–∏")
                return None
                
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF —Ñ–∞–π–ª–∞ {uploaded_file.name}: {str(e)}")
            return None
    
    def parse_product_info(self, product_string):
        try:
            product_str = str(product_string)
            price_match = re.search(r'(\d+(?:\s?\d+)*)\s*—Ä—É–±', product_str)
            price = price_match.group(1).replace(' ', '') if price_match else "0"
            return {'name': product_str, 'price': int(price)}
        except:
            return {'name': str(product_string), 'price': 0}

class AISearch:
    def __init__(self, ollama_client, data_loader):
        self.ollama = ollama_client
        self.data_loader = data_loader
    
    def find_products(self, query):
        if not self.ollama.is_available():
            st.error("Ollama –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: ollama serve")
            return []
        
        if self.data_loader.products_df is None:
            st.error("–ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ —Ç–æ–≤–∞—Ä–∞—Ö")
            return []
        
        products_list = [str(p) for p in self.data_loader.products_df['–¢–æ–≤–∞—Ä'].head(100)]
        products_text = "\n".join([f"{i+1}. {p}" for i, p in enumerate(products_list)])
        
        prompt = f"""
        –ó–∞–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞: "{query}"
        
        –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã:
        {products_text}
        
        –í—ã–±–µ—Ä–∏ –æ—Ç 3 –¥–æ 8 –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.
        –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∑–∞–ø—Ä–æ—Å –∏ –æ–ø—Ä–µ–¥–µ–ª–∏ –∫–∞–∫–∏–µ —Ç–∏–ø—ã —Ç–æ–≤–∞—Ä–æ–≤ –Ω—É–∂–Ω—ã.
        –ó–∞—Ç–µ–º –Ω–∞–π–¥–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º.
        
        –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –Ω–æ–º–µ—Ä–∞ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é –≤ —Ñ–æ—Ä–º–∞—Ç–µ: 1, 3, 5
        –ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏–∫–∞–∫–æ–≥–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
        """
        
        response = self.ollama.generate_response(prompt)
        
        if response:
            numbers = []
            number_patterns = [
                r'\b\d+\b',
                r'‚Ññ\s*(\d+)',
                r'–Ω–æ–º–µ—Ä\s*(\d+)',
            ]
            
            for pattern in number_patterns:
                found = re.findall(pattern, response)
                numbers.extend(found)
            
            selected_indices = []
            for num in set(numbers):
                if num.isdigit():
                    idx = int(num) - 1
                    if 0 <= idx < len(products_list):
                        selected_indices.append(idx)
            
            if selected_indices:
                matches = []
                for idx in selected_indices[:8]:
                    product_str = products_list[idx]
                    product_info = self.data_loader.parse_product_info(product_str)
                    matches.append(product_info)
                return matches
        
        return self._fallback_search(query, products_list)
    
    def _fallback_search(self, query, products_list):
        query_words = query.lower().split()
        matches = []
        
        for i, product_str in enumerate(products_list):
            product_lower = product_str.lower()
            score = 0
            
            for word in query_words:
                if len(word) > 2 and word in product_lower:
                    score += 1
                    if word in ['–∫–æ—Ä–æ–±', '–∫—Ä—ã—à–∫–∞', '–≥–∞–π–∫–∞', '–≤–∏–Ω—Ç', '–ª–æ—Ç–æ–∫', '—Ö–æ–º—É—Ç']:
                        score += 2
            
            if score > 0:
                product_info = self.data_loader.parse_product_info(product_str)
                matches.append((score, product_info))
        
        matches.sort(key=lambda x: x[0], reverse=True)
        return [product for score, product in matches[:5]]

class DocumentGenerator:
    def generate_tcp_document(self, order_data, products):
        doc = Document()
        
        order_paragraph = doc.add_paragraph()
        order_paragraph.add_run("–ó–∞–∫–∞–∑ ‚Ññ ").bold = True
        order_paragraph.add_run(f"{order_data['order_id']}")
        
        doc.add_paragraph("–ü–µ—Ä–µ—á–µ–Ω—å –∫–æ–º–ø–ª–µ–∫—Ç—É—é—â–∏—Ö:")
        
        total_cost = 0
        for product in products:
            p = doc.add_paragraph()
            p.add_run(f"{product['name']}")
            total_cost += product['price']
        
        price_paragraph = doc.add_paragraph()
        price_paragraph.add_run("–ò—Ç–æ–≥–æ–≤–∞—è —Ü–µ–Ω–∞: ").bold = True
        price_paragraph.add_run(f"{total_cost} —Ä—É–±.")
        
        return doc, total_cost

class JSONProcessor:
    def __init__(self, ai_search, doc_generator):
        self.ai_search = ai_search
        self.doc_generator = doc_generator
    
    def process_json_file(self, json_file):
        try:
            # –ß–∏—Ç–∞–µ–º –∏ –ø–∞—Ä—Å–∏–º JSON
            json_data = json.load(json_file)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã JSON
            if isinstance(json_data, list):
                # –ï—Å–ª–∏ JSON –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –º–∞—Å—Å–∏–≤ –∑–∞–ø—Ä–æ—Å–æ–≤
                return self._process_multiple_queries(json_data)
            elif isinstance(json_data, dict):
                # –ï—Å–ª–∏ JSON –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å
                if 'query' in json_data:
                    return self._process_single_query(json_data)
                elif 'queries' in json_data:
                    # –ï—Å–ª–∏ –µ—Å—Ç—å –ø–æ–ª–µ queries —Å –º–∞—Å—Å–∏–≤–æ–º –∑–∞–ø—Ä–æ—Å–æ–≤
                    return self._process_multiple_queries(json_data['queries'])
                else:
                    st.error("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ JSON: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–µ 'query' –∏–ª–∏ 'queries'")
                    return []
            else:
                st.error("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç JSON —Ñ–∞–π–ª–∞")
                return []
                
        except json.JSONDecodeError as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {str(e)}")
            return []
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ JSON —Ñ–∞–π–ª–∞: {str(e)}")
            return []
    
    def _process_single_query(self, query_data):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –∏–∑ JSON"""
        query_text = query_data.get('query', '')
        query_id = query_data.get('id', 'unknown')
        
        if not query_text:
            st.error("–í JSON –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ (–ø–æ–ª–µ 'query')")
            return []
        
        with st.spinner(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ {query_id}: {query_text}"):
            matches = self.ai_search.find_products(query_text)
            
            if not matches:
                st.warning(f"–î–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query_text}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤")
                return []
            
            # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –¢–ö–ü
            order_data = {'order_id': f"JSON-{query_id}"}
            doc, total_cost = self.doc_generator.generate_tcp_document(order_data, matches)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ BytesIO
            doc_buffer = io.BytesIO()
            doc.save(doc_buffer)
            doc_buffer.seek(0)
            
            return [{
                'query_id': query_id,
                'query_text': query_text,
                'matches': matches,
                'total_cost': total_cost,
                'document': doc_buffer,
                'filename': f"tcp_json_{query_id}.docx"
            }]
    
    def _process_multiple_queries(self, queries):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –º–∞—Å—Å–∏–≤–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ JSON"""
        results = []
        
        for query_data in queries:
            if isinstance(query_data, dict) and 'query' in query_data:
                result = self._process_single_query(query_data)
                results.extend(result)
            else:
                st.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç –≤ –º–∞—Å—Å–∏–≤–µ –∑–∞–ø—Ä–æ—Å–æ–≤: {query_data}")
        
        return results

def main():
    st.set_page_config(page_title="Auto-TKP", layout="centered")
    st.title("Auto-TKP –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä")
    
    ollama_client = OllamaClient()
    data_loader = DataLoader()
    ai_search = AISearch(ollama_client, data_loader)
    doc_generator = DocumentGenerator()
    json_processor = JSONProcessor(ai_search, doc_generator)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞—Ç–∞–ª–æ–≥–∞ —Ç–æ–≤–∞—Ä–æ–≤
    st.header("1. –ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞—Ç–∞–ª–æ–≥–∞ —Ç–æ–≤–∞—Ä–æ–≤")
    uploaded_files = st.file_uploader(
        "–í—ã–±–µ—Ä–∏—Ç–µ CSV –∏–ª–∏ PDF —Ñ–∞–π–ª—ã —Å —Ç–æ–≤–∞—Ä–∞–º–∏",
        type=['csv', 'pdf'],
        accept_multiple_files=True,
        key="catalog_uploader"
    )
    
    if uploaded_files:
        if data_loader.load_products_from_files(uploaded_files):
            st.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(data_loader.products_df)}")
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –Ω–∞ –¥–≤–µ –≤–∫–ª–∞–¥–∫–∏
    tab1, tab2 = st.tabs(["üìù –†—É—á–Ω–æ–π –≤–≤–æ–¥ –∑–∞–ø—Ä–æ—Å–∞", "üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ JSON –∑–∞–ø—Ä–æ—Å–æ–≤"])
    
    with tab1:
        st.header("2. –†—É—á–Ω–æ–π –≤–≤–æ–¥ –∑–∞–ø—Ä–æ—Å–∞")
        customer_query = st.text_area(
            "–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤:",
            placeholder="–ú–Ω–µ –Ω—É–∂–µ–Ω –ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã–π —Ç—Ä—É–±–æ–ø—Ä–æ–≤–æ–¥ –¥–ª–∏–Ω–æ–π 10 –º –∏ –¥–∏–∞–º–µ—Ç—Ä–æ–º –Ω–µ –º–µ–Ω–µ–µ 7 —Å–º",
            height=80,
            key="manual_query"
        )
        
        order_number = st.text_input("–ù–æ–º–µ—Ä –∑–∞–∫–∞–∑–∞", value=f"TKP-{datetime.now().strftime('%Y%m%d')}", key="manual_order")
        
        if st.button("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¢–ö–ü", type="primary", use_container_width=True, key="manual_button"):
            if not uploaded_files:
                st.error("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ CSV –∏–ª–∏ PDF —Ñ–∞–π–ª—ã —Å —Ç–æ–≤–∞—Ä–∞–º–∏")
            
            if not customer_query.strip():
                st.error("–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å")
            
            with st.spinner("–ü–æ–∏—Å–∫ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ç–æ–≤–∞—Ä–æ–≤..."):
                matches = ai_search.find_products(customer_query)
            
            if not matches:
                st.error("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ç–æ–≤–∞—Ä–æ–≤. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É—Ç–æ—á–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å.")
            
            st.write(f"–ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(matches)}")
            
            with st.spinner("–°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞..."):
                order_data = {'order_id': order_number}
                doc, total_cost = doc_generator.generate_tcp_document(order_data, matches)
                
                doc_buffer = io.BytesIO()
                doc.save(doc_buffer)
                doc_buffer.seek(0)
            
            st.success(f"–¢–ö–ü –≥–æ—Ç–æ–≤! –°—Ç–æ–∏–º–æ—Å—Ç—å: {total_cost} —Ä—É–±.")
            
            st.download_button(
                label="–°–∫–∞—á–∞—Ç—å –¢–ö–ü (tcp.docx)",
                data=doc_buffer,
                file_name="tcp.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                type="primary",
                use_container_width=True
            )
    
    with tab2:
        st.header("2. –û–±—Ä–∞–±–æ—Ç–∫–∞ JSON –∑–∞–ø—Ä–æ—Å–æ–≤")
        st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ JSON —Ñ–∞–π–ª —Å –∑–∞–ø—Ä–æ—Å–∞–º–∏. –§–æ—Ä–º–∞—Ç JSON –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø–æ–ª–µ 'query' —Å —Ç–µ–∫—Å—Ç–æ–º –∑–∞–ø—Ä–æ—Å–∞.")
        
        uploaded_json_files = st.file_uploader(
            "–í—ã–±–µ—Ä–∏—Ç–µ JSON —Ñ–∞–π–ª—ã —Å –∑–∞–ø—Ä–æ—Å–∞–º–∏",
            type=['json'],
            accept_multiple_files=True,
            key="json_uploader"
        )
        
        if uploaded_json_files:
            if not uploaded_files:
                st.error("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∫–∞—Ç–∞–ª–æ–≥ —Ç–æ–≤–∞—Ä–æ–≤ (CSV/PDF —Ñ–∞–π–ª—ã)")
            else:
                if st.button("–û–±—Ä–∞–±–æ—Ç–∞—Ç—å JSON –∑–∞–ø—Ä–æ—Å—ã", type="primary", use_container_width=True, key="json_button"):
                    all_results = []
                    
                    for json_file in uploaded_json_files:
                        st.write(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {json_file.name}")
                        results = json_processor.process_json_file(json_file)
                        all_results.extend(results)
                    
                    if all_results:
                        st.success(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {len(all_results)}")
                        
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                        for result in all_results:
                            with st.expander(f"–ó–∞–ø—Ä–æ—Å {result['query_id']}: {result['query_text']}"):
                                st.write(f"–ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(result['matches'])}")
                                st.write(f"–û–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å: {result['total_cost']} —Ä—É–±.")
                                
                                st.download_button(
                                    label=f"–°–∫–∞—á–∞—Ç—å –¢–ö–ü –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ {result['query_id']}",
                                    data=result['document'],
                                    file_name=result['filename'],
                                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                                    key=f"download_{result['query_id']}"
                                )
                    else:
                        st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –∏–∑ JSON —Ñ–∞–π–ª–æ–≤")

if __name__ == "__main__":
    main()